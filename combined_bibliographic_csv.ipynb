{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR CORE PUBLICATION REFERENCE: Baghizadeh2020\n",
      "Mähring M., Keil M., Information technology project escalation: A process model - has no references\n",
      "ERROR CORE PUBLICATION REFERENCE: Baghizadeh2020\n",
      "Bartis E., Mitev N., A multiple narrative approach to information systems failure: A successful system that failed - has no references\n",
      "ERROR CORE PUBLICATION REFERENCE: Baghizadeh2020\n",
      "Kasi V., Keil M., Mathiassen L., Pedersen K., The post mortem paradox: A Delphi study of IT specialist perceptions - has no references\n",
      "ERROR CORE PUBLICATION REFERENCE: Baghizadeh2020\n",
      "Keil Mark, Montealegre Ramiro, Cutting your losses: Extricating your organization when a big project goes awry - has no references\n",
      "ERROR CORE PUBLICATION REFERENCE: Baghizadeh2020\n",
      "Keil Mark, Mann Joan, Understanding the nature and extent of IS project escalation: Results from a survey of IS audit and control professionals - has no references\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1c/htbhjvk16vd_j4rkzt9ghj280000gn/T/ipykernel_65762/3368413811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     77\u001b[0m                         \u001b[0mread_obj_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                         \u001b[0mreference_compare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malphanumeric_filter_lower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mrow_child\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_dict_reader_children\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m                             \u001b[0mtitle_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                             \u001b[0mauthor_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_child\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\ufeffAuthors'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/csv.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;31m# Used only for its side effect.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This script creates the final combined csv, where all core publications \n",
    "# and their references are included with distinct retrieveable attributes in each column.\n",
    "# The manually extracted data is also integrated in the end.\n",
    "\n",
    "from csv import reader\n",
    "import csv\n",
    "from csv import DictReader\n",
    "import re\n",
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "papers = ['Baghizadeh2020', 'D_Arcy2011', 'Günther2017','Moeini2019', \n",
    "          'Oehlhorn2020', 'Peireira2020', 'Piccoli2005', 'Schneider2014',\n",
    "          'Siponen2004', 'Jiang2021', 'Teubner2020', 'Tsai2017',\n",
    "          'Wiener2020', 'Xiao2013']\n",
    "\n",
    "\n",
    "# first the plain reference list is coupled with the \n",
    "# relational file of core publications citing the references to integrate all\n",
    "# publications indexed by Scopus in the final CSV \n",
    "\n",
    "# additionally match the year and author for titles lesser than 5 words\n",
    "def fuzzy_classification_author_year (author, year, reference_compare):\n",
    "    first_author = author.split(\",\")[0]\n",
    "    first_author = alphanumeric_filter_lower(first_author)\n",
    "    if first_author not in reference_compare:\n",
    "        if fuzz.partial_ratio(first_author, reference_compare) < 70:\n",
    "            if author == \"[No author name available]\":\n",
    "                if year_child not in reference_compare:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def alphanumeric_filter_lower(string):\n",
    "    alphanumeric_filter = filter(str.isalnum, string)\n",
    "    string = \"\".join(alphanumeric_filter)\n",
    "    string = string.lower()\n",
    "    return string\n",
    "    \n",
    "\n",
    "for file in papers:\n",
    "    #creating combined csv\n",
    "    with open(\"./papers/\" + file + \"/\" + file + \"_joined\" + '.csv', 'w') as joined_csv:\n",
    "        writer = csv.writer(joined_csv)\n",
    "        writer.writerow([\"Author\", \"Title\", \"Year\", \"Source Title\", \"Document Type\", \"FAMILY_TYPE\"])\n",
    "        #iterating through parent and child documents (relational csv and reference csv)\n",
    "        with open(\"./papers/\" + file + \"/scopus_parents.csv\", \"r\") as read_obj_parents:\n",
    "            csv_dict_reader_parents = DictReader(read_obj_parents)\n",
    "            with open(\"./papers/\" + file + \"/scopus_children.csv\", \"r\") as read_obj_children:\n",
    "                csv_dict_reader_children = DictReader(read_obj_children)\n",
    "\n",
    "                # for each core publication in the relational file\n",
    "                for row_parent in csv_dict_reader_parents:\n",
    "\n",
    "                    author_parent = row_parent['\\ufeffAuthors']\n",
    "                    title_parent = row_parent['Title']\n",
    "                    year_parent = row_parent['Year']\n",
    "                    source_title_parent = row_parent['Source title']\n",
    "                    document_type_parent = row_parent['Document Type']\n",
    "                    references_parent = row_parent['References']\n",
    "                    already_appended_titles = []\n",
    "\n",
    "                    reference_counter = 0\n",
    "                    \n",
    "                    # if reference list is empty -> show warning -> these articles will be manually integrated\n",
    "                    if references_parent == \"\":\n",
    "                        print(\"ERROR CORE PUBLICATION REFERENCE: \" + file)\n",
    "                        print(author_parent + \", \" + title_parent + \" - has no references\")\n",
    "                        continue\n",
    "\n",
    "                    writer.writerow([author_parent, title_parent, year_parent, source_title_parent, document_type_parent, \"PARENT\"])\n",
    "                    \n",
    "                    # go through all references in the reference list and add the ones \n",
    "                    # that match to the according references in the relational file\n",
    "                    for reference in references_parent.split(\";\"):\n",
    "                        read_obj_children.seek(0)\n",
    "                        reference_compare = alphanumeric_filter_lower(reference) \n",
    "                        for row_child in csv_dict_reader_children:\n",
    "                            title_child = row_child[\"Title\"]\n",
    "                            author_child = row_child['\\ufeffAuthors']\n",
    "                            year_child = row_child['Year']\n",
    "                            if title_child != \"[No title available]\":\n",
    "                                title_compare = alphanumeric_filter_lower(title_child)\n",
    "                                if title_compare in reference_compare:\n",
    "                                    if len(title_child.split()) < 5:\n",
    "                                        if fuzzy_classification_author_year (author_child, year_child, reference_compare) == False:\n",
    "                                            continue\n",
    "                                else: \n",
    "                                    continue\n",
    "                                if title_compare not in already_appended_titles:\n",
    "                                    reference_counter += 1\n",
    "                                    already_appended_titles.append(title_compare)\n",
    "                                    source_title_child = row_child['Source title']\n",
    "                                    document_type_child = row_child['Document Type']\n",
    "                                    writer.writerow([author_child, title_child, year_child, source_title_child, document_type_child, \"CHILD\"])\n",
    "                                    break\n",
    "\n",
    "                    writer.writerow([\"Reference Count: \" + str(reference_counter), \"\", \"\", \"\", \"\", \"\"])\n",
    "                    writer.writerow([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "\n",
    "        read_obj_parents.close()\n",
    "        read_obj_children.close()\n",
    "\n",
    "        # here the manually exracted data is integrated in the combined CSV file\n",
    "        author_pattern_sublime = '(?<=<a>)(.*?)(?=</a>)'\n",
    "        pattern_sublime = '(?<=<t>)(.*?)(?=</t>)'\n",
    "        pattern_scraping = '(?<=<t>)(.*?)(?=<t>)'\n",
    "        reference_counter = 0\n",
    "\n",
    "        def find_year(line):\n",
    "            return re.findall(\"\\d{4}\", line)\n",
    "\n",
    "        def same_dates(dates):\n",
    "            return len(set(dates)) == 1\n",
    "\n",
    "        def classified(line):\n",
    "            if '<t>' in line:\n",
    "                return True\n",
    "\n",
    "        def year_filter(years):\n",
    "            years_filtered = []\n",
    "            for year in years:\n",
    "                    if 1900 <= int(year) <= 2021:\n",
    "                        years_filtered.append(year)\n",
    "            return years_filtered\n",
    "\n",
    "        def get_author(line):\n",
    "            author_match = None\n",
    "            author_match = re.search(author_pattern_sublime, line)\n",
    "            author = author_match.group(0)\n",
    "            return author\n",
    "\n",
    "        def get_title(line):\n",
    "            title_match = None\n",
    "            if ('</t>' in line):\n",
    "                # tags created with sublime\n",
    "                title_match = re.search(pattern_sublime, line)\n",
    "            else:\n",
    "                #tags created with the scraping algorithm\n",
    "                title_match = re.search(pattern_scraping, line)\n",
    "            if title_match == None:\n",
    "                print(\"no closing tag: \" + line)\n",
    "            title = title_match.group(0)\n",
    "            return title\n",
    "\n",
    "        # go through each manually added line and add publications to the combined CSV file\n",
    "        with open(\"./papers/\" + file + \"/\" + file + \"_manually_added\", \"r\") as read_obj_default:\n",
    "            for line in read_obj_default:\n",
    "                if '<PARENTARTICLE>' in line:  \n",
    "                    if reference_counter > 0:\n",
    "                        writer.writerow([\"Reference Count: \" + str(reference_counter), \"\", \"\", \"\", \"\", \"\"])\n",
    "                        writer.writerow([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "                if '<t>' in line:\n",
    "                    title = get_title(line)\n",
    "                    if title != \"\":\n",
    "\n",
    "                        year = \"\"\n",
    "                        cutted_line = line.replace(get_title(line), \"\")\n",
    "                        years_unfiltered = find_year(cutted_line)\n",
    "                        years_filtered = year_filter(years_unfiltered)\n",
    "                        if(len(years_filtered) == 0):\n",
    "                            print('Error - no year: ' + line)\n",
    "                        if (len(years_filtered) > 0):\n",
    "                            year = years_filtered[0]\n",
    "\n",
    "                        author = \"[No author name available]\"\n",
    "                        if '<a>' in line:\n",
    "                            author = get_author(line)\n",
    "\n",
    "                        if '<PARENTARTICLE>' in line:                   \n",
    "                            reference_counter = 0\n",
    "                            writer.writerow([author, title, year, line, \"\", \"PARENT\"])\n",
    "                        else: \n",
    "                            reference_counter += 1\n",
    "                            writer.writerow([author, title, year, line, \"\", \"CHILD\"])\n",
    "    \n",
    "        if reference_counter > 0:\n",
    "            writer.writerow([\"Reference Count: \" + str(reference_counter), \"\", \"\", \"\", \"\", \"\"])\n",
    "            writer.writerow([\"\", \"\", \"\", \"\", \"\", \"\"])\n",
    "    read_obj_default.close()\n",
    "    joined_csv.close()\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
